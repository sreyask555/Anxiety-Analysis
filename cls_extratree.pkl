import pandas as pd
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.metrics import classification_report, accuracy_score, f1_score
import joblib
import os

if not os.path.exists("models"):
    os.makedirs("models")

df = pd.read_csv("preprocessed_data/final_scaled_dataset.csv")
X = df.drop(columns=["Severity of Anxiety Attack (1-10)"])
y = df["Severity of Anxiety Attack (1-10)"] - 1  # Zero-indexed

X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.05, stratify=y, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=(5/95), stratify=y_train_val, random_state=42)

et = ExtraTreesClassifier(random_state=42)
param_dist_et = {
    'n_estimators': [500,700],
    'max_depth': [20,25],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2, 4],
    'criterion': ['gini', 'entropy'],
    'bootstrap': [True, False]}

random_search_et = RandomizedSearchCV(et, param_distributions=param_dist_et, n_iter=20, cv=3, scoring='f1_macro', verbose=2, n_jobs=-1, random_state=42)
random_search_et.fit(X_train, y_train)
best_et = random_search_et.best_estimator_

def evaluate(name, model, X, y):
    pred = model.predict(X)
    print(f"\n=== {name} ===")
    print("Accuracy:", accuracy_score(y, pred))
    print("F1 Score (macro):", f1_score(y, pred, average="macro"))
    print("Classification Report:\n", classification_report(y, pred))

evaluate("Train - Extra Trees", best_et, X_train, y_train)
evaluate("Validation - Extra Trees", best_et, X_val, y_val)
evaluate("Test - Extra Trees", best_et, X_test, y_test)

joblib.dump(best_et, "models/et_classifier_model.pkl")
print("Extra Trees Classifier saved.")
print("Best Extra Trees Model:\n", best_et)
