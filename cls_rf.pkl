import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import classification_report, accuracy_score, f1_score
import os
import joblib

df = pd.read_csv("preprocessed_data/cls_preprocessed_dataset.csv")

X = df.drop(columns=["Severity of Anxiety Attack (1-10)"])
y = df["Severity of Anxiety Attack (1-10)"]

X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.05, random_state=42, stratify=y)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=(5 / 95), random_state=42, stratify=y_train_val)

param_dist = {
    'n_estimators': [500, 600],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

rf = RandomForestClassifier(random_state=42, class_weight='balanced')
random_search = RandomizedSearchCV(
    rf, param_distributions=param_dist, n_iter=20, cv=3,
    scoring='f1_macro', verbose=2, n_jobs=-1, random_state=42
)
random_search.fit(X_train, y_train)

best_rf = random_search.best_estimator_

def evaluate(name, model, X, y):
    preds = model.predict(X)
    print(f"\n{name}")
    print("Accuracy:", accuracy_score(y, preds))
    print("F1-score (macro):", f1_score(y, preds, average='macro'))
    print("Classification Report:\n", classification_report(y, preds))

evaluate("Train", best_rf, X_train, y_train)
evaluate("Validation", best_rf, X_val, y_val)
evaluate("Test", best_rf, X_test, y_test)

if not os.path.exists("models"):
    os.makedirs("models")

joblib.dump(best_rf, "models/rf_classifier_model.pkl")
print("Random Forest Classifier saved.")
print("Best Parameters Found:", random_search.best_params_)
